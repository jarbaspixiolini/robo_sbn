# Guia de Instala√ß√£o - Ollama para Windows

## O que √© Ollama?

Ollama √© uma plataforma que permite executar modelos de linguagem grandes (LLMs) localmente no seu computador, sem depend√™ncia de APIs na nuvem.

---

## üì• Passo 1: Download

1. Acesse: **https://ollama.ai**
2. Clique em **"Download"**
3. Selecione **"Windows"**
4. Fa√ßa o download do instalador `.exe`

---

## üíæ Passo 2: Instala√ß√£o

1. Abra o arquivo baixado (`OllamaSetup.exe`)
2. Siga o assistente de instala√ß√£o
3. Escolha a pasta de destino (padr√£o: `C:\Users\[seu_usuario]\AppData\Local\Programs\Ollama`)
4. Clique em **"Install"**
5. Aguarde a conclus√£o

---

## ‚úÖ Passo 3: Verificar Instala√ß√£o

Abra o **PowerShell** e execute:

```powershell
ollama --version
```

Se retornar a vers√£o, est√° instalado corretamente.

---

## üöÄ Passo 4: Baixar um Modelo

No PowerShell, execute:

```powershell
ollama pull llama2
```

Ou outro modelo:
```powershell
ollama pull mistral
ollama pull neural-chat
ollama pull dolphin-mixtral
```

‚è≥ **Nota:** O download pode levar alguns minutos dependendo do tamanho do modelo.

---

## üéØ Passo 5: Executar o Modelo

```powershell
ollama run llama2
```

Agora voc√™ pode digitar perguntas e receber respostas locais!

---

## üîó Passo 6: Integra√ß√£o com Continue.dev (Opcional)

Para usar Ollama com Continue no VS Code:

1. Abra **Continue** (`Ctrl+L`)
2. V√° para **Settings** (engrenagem)
3. Configure o modelo local:

```yaml
models:
  - name: "Ollama Local"
    provider: "ollama"
    model: "llama2"
    apiBase: "http://localhost:11434"
```

---

## üìã Modelos Dispon√≠veis

| Modelo | Tamanho | Velocidade | Qualidade |
|--------|---------|-----------|-----------|
| **llama2** | 3.5 GB | R√°pido | Boa |
| **mistral** | 5 GB | Muito R√°pido | √ìtima |
| **neural-chat** | 5 GB | R√°pido | Excelente |
| **dolphin-mixtral** | 26 GB | Moderado | Excelente |
| **codellama** | 3.5 GB | R√°pido | √ìtima (c√≥digo) |

---

## üõ†Ô∏è Troubleshooting

### "Comando ollama n√£o encontrado"
- Reinicie o PowerShell ap√≥s instalar
- Verifique se Ollama foi instalado corretamente
- Tente usar o caminho completo: `C:\Users\[seu_usuario]\AppData\Local\Programs\Ollama\ollama.exe`

### "Modelo n√£o baixa"
- Verifique sua conex√£o de internet
- Tente com um modelo menor primeiro (llama2)

### "Porta 11434 j√° em uso"
- Outra inst√¢ncia do Ollama est√° rodando
- Feche outras janelas de PowerShell com Ollama
- Reinicie seu computador

---

## üéì Pr√≥ximos Passos

1. Experimente rodar o modelo: `ollama run llama2`
2. Integre com Continue.dev para usar IA no editor
3. Explore outros modelos conforme necess√°rio

---

**Ollama instalado e pronto para uso! üöÄ**
